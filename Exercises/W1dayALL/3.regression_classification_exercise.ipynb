{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "c49414725a10925ce64f1a7c91399f6a4823416942062340df486dba231dfa6d"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nivel intermedio:\n",
    "\n",
    "Hacer con objetivo de preparar entrevista técnica y tener un código reutilizable, útil y funcional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. \n",
    "\n",
    "Crea un programa que pregunte al usuario qué tipo de algoritmo desea utilizar (entre regresión lineal, regresión logística y Knn).\n",
    "\n",
    "Se presupone que el usuario proporcionará un dataframe con los datos a entrenar, el nombre de la columna target que está en el dataframe.\n",
    "\n",
    "El programa debe contener, como mínimo, estas tres funciones tal que así:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_model(option_user, params=None):\n",
    "    # your code\n",
    "    if option_user == 1:\n",
    "        model = LinearRegression()\n",
    "    elif option_user == 2:\n",
    "        model = RandomForestRegressor()\n",
    "    elif option_user == 3:\n",
    "        model = XGBRegressor()\n",
    "    elif option_user == 4:\n",
    "        model = LogisticRegression()\n",
    "    else:\n",
    "        raise Exception(\"Actualmente solo disponemos de cinco opciones [regresion linear, regresión logística, knn, Random Fores y XGBoost Regressor]\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\"k\":3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "choose_model(option_user=4, params=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Help on XGBRegressor in module xgboost.sklearn object:\n\nclass XGBRegressor(XGBModel, sklearn.base.RegressorMixin)\n |  XGBRegressor(*, objective='reg:squarederror', **kwargs)\n |  \n |  Implementation of the scikit-learn API for XGBoost regression.\n |  \n |  \n |  Parameters\n |  ----------\n |  \n |      n_estimators : int\n |          Number of gradient boosted trees.  Equivalent to number of boosting\n |          rounds.\n |  \n |      max_depth : int\n |          Maximum tree depth for base learners.\n |      learning_rate : float\n |          Boosting learning rate (xgb's \"eta\")\n |      verbosity : int\n |          The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n |      objective : string or callable\n |          Specify the learning task and the corresponding learning objective or\n |          a custom objective function to be used (see note below).\n |      booster: string\n |          Specify which booster to use: gbtree, gblinear or dart.\n |      tree_method: string\n |          Specify which tree method to use.  Default to auto.  If this parameter\n |          is set to default, XGBoost will choose the most conservative option\n |          available.  It's recommended to study this option from parameters\n |          document.\n |      n_jobs : int\n |          Number of parallel threads used to run xgboost.  When used with other Scikit-Learn\n |          algorithms like grid search, you may choose which algorithm to parallelize and\n |          balance the threads.  Creating thread contention will significantly slow dowm both\n |          algorithms.\n |      gamma : float\n |          Minimum loss reduction required to make a further partition on a leaf\n |          node of the tree.\n |      min_child_weight : float\n |          Minimum sum of instance weight(hessian) needed in a child.\n |      max_delta_step : int\n |          Maximum delta step we allow each tree's weight estimation to be.\n |      subsample : float\n |          Subsample ratio of the training instance.\n |      colsample_bytree : float\n |          Subsample ratio of columns when constructing each tree.\n |      colsample_bylevel : float\n |          Subsample ratio of columns for each level.\n |      colsample_bynode : float\n |          Subsample ratio of columns for each split.\n |      reg_alpha : float (xgb's alpha)\n |          L1 regularization term on weights\n |      reg_lambda : float (xgb's lambda)\n |          L2 regularization term on weights\n |      scale_pos_weight : float\n |          Balancing of positive and negative weights.\n |      base_score:\n |          The initial prediction score of all instances, global bias.\n |      random_state : int\n |          Random number seed.\n |  \n |          .. note::\n |  \n |             Using gblinear booster with shotgun updater is nondeterministic as\n |             it uses Hogwild algorithm.\n |  \n |      missing : float, default np.nan\n |          Value in the data which needs to be present as a missing value.\n |      num_parallel_tree: int\n |          Used for boosting random forest.\n |      monotone_constraints : str\n |          Constraint of variable monotonicity.  See tutorial for more\n |          information.\n |      interaction_constraints : str\n |          Constraints for interaction representing permitted interactions.  The\n |          constraints must be specified in the form of a nest list, e.g. [[0, 1],\n |          [2, 3, 4]], where each inner list is a group of indices of features\n |          that are allowed to interact with each other.  See tutorial for more\n |          information\n |      importance_type: string, default \"gain\"\n |          The feature importance type for the feature_importances\\_ property:\n |          either \"gain\", \"weight\", \"cover\", \"total_gain\" or \"total_cover\".\n |  \n |      \\*\\*kwargs : dict, optional\n |          Keyword arguments for XGBoost Booster object.  Full documentation of\n |          parameters can be found here:\n |          https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst.\n |          Attempting to set a parameter via the constructor args and \\*\\*kwargs\n |          dict simultaneously will result in a TypeError.\n |  \n |          .. note:: \\*\\*kwargs unsupported by scikit-learn\n |  \n |              \\*\\*kwargs is unsupported by scikit-learn.  We do not guarantee\n |              that parameters passed via this argument will interact properly\n |              with scikit-learn.\n |  \n |          .. note::  Custom objective function\n |  \n |              A custom objective function can be provided for the ``objective``\n |              parameter. In this case, it should have the signature\n |              ``objective(y_true, y_pred) -> grad, hess``:\n |  \n |              y_true: array_like of shape [n_samples]\n |                  The target values\n |              y_pred: array_like of shape [n_samples]\n |                  The predicted values\n |  \n |              grad: array_like of shape [n_samples]\n |                  The value of the gradient for each sample point.\n |              hess: array_like of shape [n_samples]\n |                  The value of the second derivative for each sample point\n |  \n |  Method resolution order:\n |      XGBRegressor\n |      XGBModel\n |      sklearn.base.BaseEstimator\n |      sklearn.base.RegressorMixin\n |      builtins.object\n |  \n |  Methods defined here:\n |  \n |  __init__(self, *, objective='reg:squarederror', **kwargs)\n |      Initialize self.  See help(type(self)) for accurate signature.\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from XGBModel:\n |  \n |  apply(self, X, ntree_limit=0)\n |      Return the predicted leaf every tree for each sample.\n |      \n |      Parameters\n |      ----------\n |      X : array_like, shape=[n_samples, n_features]\n |          Input features matrix.\n |      \n |      ntree_limit : int\n |          Limit number of trees in the prediction; defaults to 0 (use all trees).\n |      \n |      Returns\n |      -------\n |      X_leaves : array_like, shape=[n_samples, n_trees]\n |          For each datapoint x in X and for each tree, return the index of the\n |          leaf x ends up in. Leaves are numbered within\n |          ``[0; 2**(self.max_depth+1))``, possibly with gaps in the numbering.\n |  \n |  evals_result(self)\n |      Return the evaluation results.\n |      \n |      If **eval_set** is passed to the `fit` function, you can call\n |      ``evals_result()`` to get evaluation results for all passed **eval_sets**.\n |      When **eval_metric** is also passed to the `fit` function, the\n |      **evals_result** will contain the **eval_metrics** passed to the `fit` function.\n |      \n |      Returns\n |      -------\n |      evals_result : dictionary\n |      \n |      Example\n |      -------\n |      \n |      .. code-block:: python\n |      \n |          param_dist = {'objective':'binary:logistic', 'n_estimators':2}\n |      \n |          clf = xgb.XGBModel(**param_dist)\n |      \n |          clf.fit(X_train, y_train,\n |                  eval_set=[(X_train, y_train), (X_test, y_test)],\n |                  eval_metric='logloss',\n |                  verbose=True)\n |      \n |          evals_result = clf.evals_result()\n |      \n |      The variable **evals_result** will contain:\n |      \n |      .. code-block:: python\n |      \n |          {'validation_0': {'logloss': ['0.604835', '0.531479']},\n |          'validation_1': {'logloss': ['0.41965', '0.17686']}}\n |  \n |  fit(self, X, y, *, sample_weight=None, base_margin=None, eval_set=None, eval_metric=None, early_stopping_rounds=None, verbose=True, xgb_model=None, sample_weight_eval_set=None, feature_weights=None, callbacks=None)\n |      Fit gradient boosting model\n |      \n |      Parameters\n |      ----------\n |      X : array_like\n |          Feature matrix\n |      y : array_like\n |          Labels\n |      sample_weight : array_like\n |          instance weights\n |      base_margin : array_like\n |          global bias for each instance.\n |      eval_set : list, optional\n |          A list of (X, y) tuple pairs to use as validation sets, for which\n |          metrics will be computed.\n |          Validation metrics will help us track the performance of the model.\n |      eval_metric : str, list of str, or callable, optional\n |          If a str, should be a built-in evaluation metric to use. See\n |          doc/parameter.rst.\n |          If a list of str, should be the list of multiple built-in evaluation metrics\n |          to use.\n |          If callable, a custom evaluation metric. The call\n |          signature is ``func(y_predicted, y_true)`` where ``y_true`` will be a\n |          DMatrix object such that you may need to call the ``get_label``\n |          method. It must return a str, value pair where the str is a name\n |          for the evaluation and value is the value of the evaluation\n |          function. The callable custom objective is always minimized.\n |      early_stopping_rounds : int\n |          Activates early stopping. Validation metric needs to improve at least once in\n |          every **early_stopping_rounds** round(s) to continue training.\n |          Requires at least one item in **eval_set**.\n |          The method returns the model from the last iteration (not the best one).\n |          If there's more than one item in **eval_set**, the last entry will be used\n |          for early stopping.\n |          If there's more than one metric in **eval_metric**, the last metric will be\n |          used for early stopping.\n |          If early stopping occurs, the model will have three additional fields:\n |          ``clf.best_score``, ``clf.best_iteration`` and ``clf.best_ntree_limit``.\n |      verbose : bool\n |          If `verbose` and an evaluation set is used, writes the evaluation\n |          metric measured on the validation set to stderr.\n |      xgb_model : str\n |          file name of stored XGBoost model or 'Booster' instance XGBoost model to be\n |          loaded before training (allows training continuation).\n |      sample_weight_eval_set : list, optional\n |          A list of the form [L_1, L_2, ..., L_n], where each L_i is a list of\n |          instance weights on the i-th validation set.\n |      feature_weights: array_like\n |          Weight for each feature, defines the probability of each feature being\n |          selected when colsample is being used.  All values must be greater than 0,\n |          otherwise a `ValueError` is thrown.  Only available for `hist`, `gpu_hist` and\n |          `exact` tree methods.\n |      callbacks : list of callback functions\n |          List of callback functions that are applied at end of each iteration.\n |          It is possible to use predefined callbacks by using :ref:`callback_api`.\n |          Example:\n |      \n |          .. code-block:: python\n |      \n |              callbacks = [xgb.callback.EarlyStopping(rounds=early_stopping_rounds,\n |                                                      save_best=True)]\n |  \n |  get_booster(self)\n |      Get the underlying xgboost Booster of this model.\n |      \n |      This will raise an exception when fit was not called\n |      \n |      Returns\n |      -------\n |      booster : a xgboost booster of underlying model\n |  \n |  get_num_boosting_rounds(self)\n |      Gets the number of xgboost boosting rounds.\n |  \n |  get_params(self, deep=True)\n |      Get parameters.\n |  \n |  get_xgb_params(self)\n |      Get xgboost specific parameters.\n |  \n |  load_model(self, fname)\n |      Load the model from a file.\n |      \n |      The model is loaded from an XGBoost internal format which is universal\n |      among the various XGBoost interfaces. Auxiliary attributes of the\n |      Python Booster object (such as feature names) will not be loaded.\n |      \n |      Parameters\n |      ----------\n |      fname : string\n |          Input file name.\n |  \n |  predict(self, data, output_margin=False, ntree_limit=None, validate_features=True, base_margin=None)\n |      Predict with `data`.\n |      \n |      .. note:: This function is not thread safe.\n |      \n |        For each booster object, predict can only be called from one thread.\n |        If you want to run prediction using multiple thread, call ``xgb.copy()`` to make copies\n |        of model object and then call ``predict()``.\n |      \n |        .. code-block:: python\n |      \n |          preds = bst.predict(dtest, ntree_limit=num_round)\n |      \n |      Parameters\n |      ----------\n |      data : numpy.array/scipy.sparse\n |          Data to predict with\n |      output_margin : bool\n |          Whether to output the raw untransformed margin value.\n |      ntree_limit : int\n |          Limit number of trees in the prediction; defaults to best_ntree_limit if defined\n |          (i.e. it has been trained with early stopping), otherwise 0 (use all trees).\n |      validate_features : bool\n |          When this is True, validate that the Booster's and data's feature_names are identical.\n |          Otherwise, it is assumed that the feature_names are the same.\n |      Returns\n |      -------\n |      prediction : numpy array\n |  \n |  save_model(self, fname: str)\n |      Save the model to a file.\n |      \n |      The model is saved in an XGBoost internal format which is universal\n |      among the various XGBoost interfaces. Auxiliary attributes of the\n |      Python Booster object (such as feature names) will not be saved.\n |      \n |        .. note::\n |      \n |          See:\n |      \n |          https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html\n |      \n |      Parameters\n |      ----------\n |      fname : string\n |          Output file name\n |  \n |  set_params(self, **params)\n |      Set the parameters of this estimator.  Modification of the sklearn method to\n |      allow unknown kwargs. This allows using the full range of xgboost\n |      parameters that are not defined as member variables in sklearn grid\n |      search.\n |      \n |      Returns\n |      -------\n |      self\n |  \n |  ----------------------------------------------------------------------\n |  Readonly properties inherited from XGBModel:\n |  \n |  coef_\n |      Coefficients property\n |      \n |      .. note:: Coefficients are defined only for linear learners\n |      \n |          Coefficients are only defined when the linear model is chosen as\n |          base learner (`booster=gblinear`). It is not defined for other base\n |          learner types, such as tree learners (`booster=gbtree`).\n |      \n |      Returns\n |      -------\n |      coef_ : array of shape ``[n_features]`` or ``[n_classes, n_features]``\n |  \n |  feature_importances_\n |      Feature importances property\n |      \n |      .. note:: Feature importance is defined only for tree boosters\n |      \n |          Feature importance is only defined when the decision tree model is chosen as base\n |          learner (`booster=gbtree`). It is not defined for other base learner types, such\n |          as linear learners (`booster=gblinear`).\n |      \n |      Returns\n |      -------\n |      feature_importances_ : array of shape ``[n_features]``\n |  \n |  intercept_\n |      Intercept (bias) property\n |      \n |      .. note:: Intercept is defined only for linear learners\n |      \n |          Intercept (bias) is only defined when the linear model is chosen as base\n |          learner (`booster=gblinear`). It is not defined for other base learner types, such\n |          as tree learners (`booster=gbtree`).\n |      \n |      Returns\n |      -------\n |      intercept_ : array of shape ``(1,)`` or ``[n_classes]``\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from sklearn.base.BaseEstimator:\n |  \n |  __getstate__(self)\n |  \n |  __repr__(self, N_CHAR_MAX=700)\n |      Return repr(self).\n |  \n |  __setstate__(self, state)\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors inherited from sklearn.base.BaseEstimator:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from sklearn.base.RegressorMixin:\n |  \n |  score(self, X, y, sample_weight=None)\n |      Return the coefficient of determination :math:`R^2` of the\n |      prediction.\n |      \n |      The coefficient :math:`R^2` is defined as :math:`(1 - \\frac{u}{v})`,\n |      where :math:`u` is the residual sum of squares ``((y_true - y_pred)\n |      ** 2).sum()`` and :math:`v` is the total sum of squares ``((y_true -\n |      y_true.mean()) ** 2).sum()``. The best possible score is 1.0 and it\n |      can be negative (because the model can be arbitrarily worse). A\n |      constant model that always predicts the expected value of `y`,\n |      disregarding the input features, would get a :math:`R^2` score of\n |      0.0.\n |      \n |      Parameters\n |      ----------\n |      X : array-like of shape (n_samples, n_features)\n |          Test samples. For some estimators this may be a precomputed\n |          kernel matrix or a list of generic objects instead with shape\n |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n |          is the number of samples used in the fitting for the estimator.\n |      \n |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n |          True values for `X`.\n |      \n |      sample_weight : array-like of shape (n_samples,), default=None\n |          Sample weights.\n |      \n |      Returns\n |      -------\n |      score : float\n |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n |      \n |      Notes\n |      -----\n |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n |      with default value of :func:`~sklearn.metrics.r2_score`.\n |      This influences the ``score`` method of all the multioutput\n |      regressors (except for\n |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n\n"
     ]
    }
   ],
   "source": [
    "help(choose_model(option_user=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------\n",
    "\"choose_model\" recibe:\n",
    "\n",
    "- 'option_user': la opción del usuario. \n",
    "- 'params': es un diccionario que puede contener ciertos parámetros necesarios para la creación de los modelos (por ejemplo, el valor k para el algoritmo Knn). Por defecto, su valor es None.\n",
    "\n",
    "Deberá crear el modelo necesario sin entrenar, retornándolo al final.\n",
    "\n",
    "----------------\n",
    "\n",
    "\"train_model\" recibe:\n",
    "\n",
    "- 'model': el modelo sin entrenar elegido por el usario\n",
    "- 'df': el dataframe tratado y limpio que contiene todos los datos del conjunto de entrenamiento y de test, incluyendo el target. \n",
    "- 'target_name': el nombre de la columna que representa el target.\n",
    "\n",
    "Retornará el modelo entrenado con los datos y la precisión del modelo.\n",
    "\n",
    "\n",
    "----------------\n",
    "\n",
    "\"main\": \n",
    "\n",
    "Es la función que ha de ser ejecutada cada vez que queremos lanzar el programa. \n",
    "\n",
    "Al final, mostrará la precisión del modelo entrenado y retornará el modelo entrenado.\n",
    "\n",
    "----------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Haz que el programa sea capaz de predecir un nuevo ejemplo del modelo elegido y entrenado con la siguiente función:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_new_data(model_trained, to_pred):\n",
    "    \n",
    "    # your code\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "\n",
    "\"predict_new_data\" recibe:\n",
    "\n",
    "- 'model_trained': el modelo entrenado elegido por el usario\n",
    "- 'to_pred': los datos del nuevo ejemplo a predecir.\n",
    "\n",
    "Retornará y mostrará por pantalla la predicción.\n",
    "\n",
    "----------------\n",
    "\n",
    "Se presupone que el usuario proporcionará los datos del nuevo ejemplo."
   ]
  }
 ]
}